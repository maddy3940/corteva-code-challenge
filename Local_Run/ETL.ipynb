{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from functools import wraps\n",
    "from datetime import datetime\n",
    "import inspect\n",
    "import shutil\n",
    "import subprocess\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorators\n",
    "def execution_time_log():\n",
    "    def inner_decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                start_time = datetime.now()\n",
    "                result = func(*args, **kwargs)\n",
    "                end_time = datetime.now()\n",
    "                execution_time = end_time - start_time\n",
    "                logging.info(f\"The function {func.__name__} - Ran for {execution_time} seconds\")    \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logging.info(f\"The function {func.__name__} encountered an error:  {e}\")  \n",
    "        return wrapper\n",
    "    return inner_decorator\n",
    "\n",
    "def error_log(function_name):\n",
    "    def inner_decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logging.info(f\"Function {function_name} encountered an error: {e}\")  \n",
    "        return wrapper\n",
    "    return inner_decorator\n",
    "\n",
    "\n",
    "def counts_log():\n",
    "    def inner_decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                logging.info(f\"The function {func.__name__}  inserted {len(result)} records\")         \n",
    "            except Exception as e:\n",
    "                logging.info(f\"The function {func.__name__} encountered an error:  {e}\")  \n",
    "        return wrapper\n",
    "    return inner_decorator\n",
    "\n",
    "\n",
    "def check_commit_time(file_name):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                try:\n",
    "                    old_commit_times = pd.read_csv(os.getcwd()+'\\Logs\\commit_time.txt',sep='\\t')\n",
    "                    old_commit_time = datetime.strptime(old_commit_times[old_commit_times['Table name']==file_name]['Commit_Time'].values[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "                except:\n",
    "                    logging.info(\"Historical commit data does not exist. Adding new commit data...\") \n",
    "                    old_commit_times = {\"Table name\":['Weather','Yield'],\"Commit_Time\":['2000-01-01 00:00:00','2000-01-01 00:00:00']}\n",
    "                    old_commit_times = pd.DataFrame(old_commit_times)\n",
    "                    old_commit_time = None\n",
    "                new_commit_time = get_last_commit_time(repo_url,weather_path) if file_name=='Weather' else get_last_commit_time(repo_url,yield_path)\n",
    "                new_commit_time = datetime.strptime(new_commit_time[:-6], \"%Y-%m-%d %H:%M:%S\") \n",
    "                \n",
    "                # Check if new commit time is greater than old commit time\n",
    "                if old_commit_time is None or new_commit_time > old_commit_time:\n",
    "                    logging.info(\"A new commit has been made to source data. Executing ingestion pipeline...\")\n",
    "                    old_commit_times.loc[old_commit_times['Table name'] == file_name, 'Commit_Time'] = new_commit_time\n",
    "                    old_commit_times.to_csv(os.getcwd()+'\\Logs\\commit_time.txt', sep='\\t', index=False)\n",
    "                    return func(*args, **kwargs)\n",
    "                else:\n",
    "                    logging.info(\"No new commit has been made to source data. No need to execute ingestion pipeline\")\n",
    "            except Exception as e:\n",
    "                logging.info(f\"The function {func.__name__} encountered an error:  {e}\")              \n",
    "                \n",
    "        return wrapper\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('get_all_file_list')\n",
    "def get_all_file_list(repo_url,api_endpoint):\n",
    "    return  requests.get(repo_url + api_endpoint).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@check_commit_time('Weather')\n",
    "def read_write_weather_cdc_data():\n",
    "    df_weather = get_weather_data()\n",
    "    weather_cdc = get_cdc_data('weather',cnx,cursor,df_weather)\n",
    "    \n",
    "    if len(weather_cdc)!=0:\n",
    "        logging.info(\"Writing into Weather table\")\n",
    "        write_df_to_db(weather_cdc,'weather')\n",
    "    return weather_cdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@check_commit_time('Yield')\n",
    "def read_write_yield_cdc_data():\n",
    "    df_yield = get_yield_data()\n",
    "    yield_cdc = get_cdc_data('yield',cnx,cursor,df_yield)\n",
    "    \n",
    "    if len(yield_cdc)!=0:\n",
    "        logging.info(\"Writing into Yield table\")\n",
    "        write_df_to_db(yield_cdc,'yield')\n",
    "    return yield_cdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@execution_time_log()\n",
    "def get_weather_data():\n",
    "\n",
    "    df_list = []\n",
    "    # Loop through the list of files and download and read each file into a dataframe\n",
    "    temp_path=os.getcwd()+'\\Temp\\wx_data'\n",
    "    for file_name in os.listdir(temp_path):        \n",
    "        df = pd.read_csv(os.path.join(temp_path, file_name), sep='\\t',names=['Date','Maximum_Temperature','Minimum_Temperature','Precipitation'])        \n",
    "        df['Station_ID']=file_name[:-4]\n",
    "        df_list.append(df)\n",
    "        break\n",
    "\n",
    "    # Concatenate the dataframes into one\n",
    "    df_all = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    df_all['hash_value'] = df_all.apply(lambda row: ''.join(str(value) for value in row.values), axis=1)\n",
    "    df_all['Date']=pd.to_datetime(df_all['Date'], format='%Y%m%d')\n",
    "    return df_all\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@execution_time_log()\n",
    "def get_yield_data():\n",
    "\n",
    "    # Yield\n",
    "    # List to hold the dataframes\n",
    "    df_yield_list = []\n",
    "    temp_path=os.getcwd()+'\\Temp\\yld_data'\n",
    "    for file_name in os.listdir(temp_path):\n",
    "        df = pd.read_csv(os.path.join(temp_path, file_name), sep='\\t',names=['Year','Yield'])\n",
    "        df_yield_list.append(df)\n",
    "\n",
    "    # Concatenate the dataframes into one\n",
    "    df_yield_all = pd.concat(df_yield_list, axis=0, ignore_index=True)\n",
    "    df_yield_all['hash_value'] = df_yield_all.apply(lambda row: ''.join(str(value) for value in row.values), axis=1)\n",
    "    return df_yield_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('get_last_commit_time')\n",
    "def get_last_commit_time(repo_url,folder_path):\n",
    "    path = \"/commits?path=\"\n",
    "\n",
    "    # Make the API request and get the response as a JSON object\n",
    "    response = requests.get(repo_url + path+ folder_path)\n",
    "    commits = response.json()\n",
    "\n",
    "    # Get the timestamp of the latest commit that modified the file\n",
    "    timestamp = commits[0]['commit']['committer']['date']\n",
    "\n",
    "    utc_dt = datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%SZ')  # Convert UTC string to datetime object\n",
    "    local_tz = pytz.timezone('US/Eastern')  # Specify the timezone you want to convert to\n",
    "    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n",
    "\n",
    "    return str(local_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@counts_log()\n",
    "def write_df_to_db(df,table_name):\n",
    "    df[:10].to_sql(table_name,con=engine, if_exists='append', index=False)\n",
    "    cursor.execute(\"COMMIT\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@execution_time_log()\n",
    "def get_cdc_data(target_table,connection,cursor,new_df):\n",
    "    subquery = f\"SELECT hash_value FROM {target_table}\"\n",
    "    cursor.execute('USE corteva')\n",
    "    try:\n",
    "        existing_hashes = pd.read_sql(subquery, connection)\n",
    "    except:\n",
    "        logging.info(f\"{target_table} table has not yet been created in the database. Creating {target_table} table..\") \n",
    "        existing_hashes = pd.DataFrame(columns=['hash_value'])\n",
    "    try:\n",
    "        cdc_df= new_df[~new_df['hash_value'].isin(existing_hashes['hash_value'])]\n",
    "        cdc_df.drop_duplicates(inplace=True)\n",
    "        return cdc_df\n",
    "    except:\n",
    "        logging.info(f\"Either all newly committed values already present in the table\") \n",
    "        return pd.DataFrame(columns=new_df.columns.tolist()+['hash_value'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('clean_df')\n",
    "def clean_df(df):\n",
    "    df = df.loc[(df['Maximum_Temperature']!=-9999) & (df['Minimum_Temperature']!=-9999) & (df['Precipitation']!=-9999)]\n",
    "    df['Year']= df['Date'].dt.year\n",
    "    df['hash_value']=df['Year'].astype(str)+df['Station_ID']\n",
    "    return df[['hash_value','Station_ID','Year','Maximum_Temperature','Minimum_Temperature','Precipitation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('get_year_station_id')\n",
    "def get_year_station_id(df):\n",
    "    distinct_hash = df['hash_value'].unique()\n",
    "    return distinct_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('get_existing_year_station_id')\n",
    "def get_existing_year_station_id():\n",
    "    query_hash = f\"SELECT distinct concat(year(Date),Station_ID) as hash_value FROM weather\"\n",
    "    cursor.execute('USE corteva')\n",
    "    try:\n",
    "        existing_hash = pd.read_sql(query_year, cnx)\n",
    "        return existing_hash\n",
    "    except:\n",
    "        return pd.DataFrame(columns=['hash_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('intersection_year_stationid_new_old')\n",
    "def intersection_year_stationid_new_old(df): \n",
    "    new_hash = get_year_station_id(df)\n",
    "    \n",
    "    existing_hash =  get_existing_year_station_id()\n",
    "    existing_hash= existing_hash['hash_value'].tolist()\n",
    "    \n",
    "    stats_hash_to_compute = [x for x in new_hash if x not in existing_hash]\n",
    "\n",
    "    return list(set(stats_hash_to_compute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('compute_new_stats')\n",
    "def compute_new_stats(df):\n",
    "    # Group the data by Station_ID and Year\n",
    "    grouped = df.groupby(['Station_ID', 'Year'])\n",
    "    \n",
    "    # Calculate the average of Maximum_Temperature, Minimum_Temperature, and Precipitation for each group\n",
    "    agg_df = grouped.agg({'Maximum_Temperature': 'mean',\n",
    "                          'Minimum_Temperature': 'mean',\n",
    "                          'Precipitation': 'sum'})\n",
    "\n",
    "    # Rename the columns to match the desired output schema\n",
    "    agg_df = agg_df.rename(columns={'Maximum_Temperature': 'Average maximum temperature',\n",
    "                                     'Minimum_Temperature': 'Average minimum temperature',\n",
    "                                     'Precipitation': 'Total accumulated precipitation'})\n",
    "    \n",
    "    # Reset the index to make Station_ID and Year columns\n",
    "    agg_df = agg_df.reset_index()\n",
    "\n",
    "    agg_df['hash_value']=agg_df['Station_ID']+ agg_df['Year'].astype('str')\n",
    "\n",
    "    return agg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('delete_old_inset_new_stats')\n",
    "def delete_old_insert_new_stats(weather_df_stats,stats_hash_to_compute):\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(stats_hash_to_compute)):\n",
    "            query = f\"\"\"DELETE FROM Weather_Stats WHERE Year = '{stats_hash_to_compute[i][0]}' and Station_ID = '{stats_hash_to_compute[i][1]}'\"\"\"\n",
    "            cursor.execute(query)\n",
    "            cursor.execute(\"COMMIT\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Weather_Stats table has not yet been created in the database. Creating Weather_Stats table..\")\n",
    "    logging.info(\"Writing into Weather Stats table\")\n",
    "    write_df_to_db(weather_df_stats,'Weather_Stats')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('filter_by_year_stationid')\n",
    "def filter_by_year_stationid(df,stats_hash_to_compute):\n",
    "    df_old=[]\n",
    "    for i in range(len(stats_hash_to_compute)):\n",
    "        query = f\"\"\"\n",
    "        SELECT Station_ID,year(date) as Year,Maximum_Temperature,Minimum_Temperature,Precipitation\n",
    "        FROM weather\n",
    "        WHERE year(date) ={str(stats_hash_to_compute[i][0])} AND Station_ID ='{stats_hash_to_compute[i][1]}'\"\"\"\n",
    "        \n",
    "        # Execute SQL query with parameters\n",
    "        df_old_temp = pd.read_sql(query, con=cnx)\n",
    "        df_old.append(df_old_temp)\n",
    "\n",
    "    try:\n",
    "        df_old = pd.concat(df_old, axis=0, ignore_index=True)\n",
    "    except:\n",
    "        df_old =  pd.DataFrame(columns=['Station_ID','Year','Maximum_Temperature','Minimum_Temperature','Precipitation'])\n",
    "    new_df = pd.concat([df[['Station_ID','Year','Maximum_Temperature','Minimum_Temperature','Precipitation']], df_old])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@error_log('compute_and_store_analysis')\n",
    "def compute_and_store_analysis(weather_df):\n",
    "    weather_df_clean = clean_df(weather_df)\n",
    "    stats_hash_to_compute = intersection_year_stationid_new_old(weather_df_clean)\n",
    "    stats_hash_to_compute = [(x[:4], x[4:]) for x in stats_hash_to_compute]\n",
    "    weather_df_clean_filtered = filter_by_year_stationid(weather_df_clean,stats_hash_to_compute)\n",
    "    weather_df_stats = compute_new_stats(weather_df_clean_filtered)\n",
    "    delete_old_insert_new_stats(weather_df_stats,stats_hash_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "# Variables\n",
    "repo_url = config['repo_url']\n",
    "weather_path = config['weather_path']\n",
    "yield_path = config['yield_path']\n",
    "weather_api_endpoint = config['weather_api_endpoint']\n",
    "yield_api_endpoint = config['yield_api_endpoint']\n",
    "url = config['url']\n",
    "# Connection\n",
    "cnx = mysql.connector.connect(\n",
    "    host=config['database']['host'],\n",
    "    user=config['database']['username'],\n",
    "    password=config['database']['password'],\n",
    "    port=config['database']['port']\n",
    ")\n",
    "cursor = cnx.cursor()\n",
    "engine = create_engine(f'mysql+mysqlconnector://{config[\"database\"][\"username\"]}:{config[\"database\"][\"password\"]}@{config[\"database\"][\"host\"]}:{config[\"database\"][\"port\"]}/corteva')\n",
    "cursor.execute('CREATE DATABASE IF NOT EXISTS corteva ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():  \n",
    "    # Inestion\n",
    "    weather_df = read_write_weather_cdc_data()\n",
    "    yield_df = read_write_yield_cdc_data()\n",
    "\n",
    "\n",
    "    # Analysis \n",
    "    if isinstance(weather_df, pd.DataFrame) and len(weather_df)!=0:\n",
    "        compute_and_store_analysis(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(filename='execution_logs.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth=1\", url, os.getcwd()+'\\Temp'])\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
